{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2c97b8d6",
      "metadata": {},
      "source": [
        "# SAM3 Video/Image-Folder Multi-Prompt Selector\n",
        "\n",
        "This notebook lets you define prompts interactively on a frame and run SAM3 tracking with them:\n",
        "\n",
        "- Multiple bounding boxes (positive box prompts)\n",
        "- Positive and negative point prompts\n",
        "- Multiple text prompts\n",
        "\n",
        "Input resource can be either:\n",
        "\n",
        "- A video file (`.mp4`, `.mov`, ...)\n",
        "- A folder of JPEG images (`.jpg` / `.jpeg`)\n",
        "\n",
        "Workflow:\n",
        "\n",
        "1. Configure resource path and frame index.\n",
        "2. Draw/select prompts in the interactive canvas.\n",
        "3. Build prompt payload.\n",
        "4. Run SAM3 and propagate masks across frames.\n",
        "\n",
        "Tips:\n",
        "\n",
        "- In image-folder mode, prompting is applied on the first frame (`FRAME_INDEX=0`).\n",
        "- Use `%matplotlib widget` for interactive drawing.\n",
        "- Keyboard shortcuts:\n",
        "  - `b`: box mode\n",
        "  - `p`: positive-point mode\n",
        "  - `n`: negative-point mode\n",
        "  - `u`: undo last annotation\n",
        "  - `c`: clear all spatial annotations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d35298a",
      "metadata": {},
      "outputs": [],
      "source": [
        "%matplotlib widget\n",
        "\n",
        "from pathlib import Path\n",
        "import tempfile\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import clear_output, display\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Rectangle\n",
        "from matplotlib.widgets import RectangleSelector\n",
        "import mediapy as media\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "from sam3.model_builder import build_sam3_video_predictor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f821452",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Configure these ---\n",
        "# INPUT_RESOURCE_PATH can be either:\n",
        "# - a video file path (e.g. .mp4, .mov), or\n",
        "# - a folder containing JPEG images.\n",
        "# INPUT_RESOURCE_PATH = \"/data/sam3_based_labeling_pipeline/assets/videos/third_view.mp4\"\n",
        "INPUT_RESOURCE_PATH = \"/data/sam3_based_labeling_pipeline/assets/test_le_robot_dataset/videos/0/1_rgb.mp4\"\n",
        "# INPUT_RESOURCE_PATH = \"/data/sam3_based_labeling_pipeline/assets/videos/test_images\"\n",
        "FRAME_INDEX = 0\n",
        "DEFAULT_OBJECT_ID = 0\n",
        "\n",
        "# Inference controls\n",
        "MAX_FRAME_NUM_TO_TRACK = 300\n",
        "OUTPUT_OVERLAY_VIDEO_PATH = \"/data/sam3_based_labeling_pipeline/assets/videos/two_orca_hands_multi_object_overlay.mp4\"\n",
        "OUTPUT_FPS = 5\n",
        "MASK_ALPHA = 1.0\n",
        "\n",
        "# Stability setting:\n",
        "# Disable hole-filling postprocessing to avoid Triton connected-components launch failures\n",
        "# (useful for high-resolution inputs, regardless of whether source is video or image folder).\n",
        "DISABLE_HOLE_FILLING = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37e3f9e2",
      "metadata": {},
      "outputs": [],
      "source": [
        "resource_path = Path(INPUT_RESOURCE_PATH)\n",
        "if not resource_path.exists():\n",
        "    raise FileNotFoundError(f\"Resource not found: {resource_path}\")\n",
        "\n",
        "\n",
        "def _ensure_rgb_uint8(img):\n",
        "    img = np.asarray(img)\n",
        "    if img.ndim == 2:\n",
        "        img = np.stack([img, img, img], axis=-1)\n",
        "    elif img.ndim == 3 and img.shape[-1] == 4:\n",
        "        img = img[:, :, :3]\n",
        "    elif img.ndim == 3 and img.shape[-1] > 4:\n",
        "        img = img[:, :, :3]\n",
        "    return np.asarray(img, dtype=np.uint8)\n",
        "\n",
        "\n",
        "def _resize_longest_side(img, target_longest=1008):\n",
        "    h, w = img.shape[:2]\n",
        "    scale = min(1.0, float(target_longest) / float(max(h, w)))\n",
        "    if scale == 1.0:\n",
        "        return img\n",
        "\n",
        "    new_w = max(1, int(round(w * scale)))\n",
        "    new_h = max(1, int(round(h * scale)))\n",
        "    pil_img = Image.fromarray(img)\n",
        "    pil_img = pil_img.resize((new_w, new_h), resample=Image.BILINEAR)\n",
        "    return np.asarray(pil_img, dtype=np.uint8)\n",
        "\n",
        "\n",
        "if resource_path.is_dir():\n",
        "    # For image-folder mode, only JPEGs are supported here by design.\n",
        "    frame_paths = [\n",
        "        p for p in resource_path.iterdir() if p.is_file() and p.suffix.lower() in {\".jpg\", \".jpeg\"}\n",
        "    ]\n",
        "    if not frame_paths:\n",
        "        raise ValueError(f\"No JPEG files found in folder: {resource_path}\")\n",
        "\n",
        "    try:\n",
        "        frame_paths.sort(key=lambda p: int(p.stem))\n",
        "    except ValueError:\n",
        "        frame_paths.sort(key=lambda p: p.name)\n",
        "\n",
        "    original_video_frames = [_ensure_rgb_uint8(media.read_image(str(p))) for p in frame_paths]\n",
        "    source_kind = \"image_folder\"\n",
        "\n",
        "    if FRAME_INDEX != 0:\n",
        "        print(\"Image-folder mode uses the first frame for prompting; overriding FRAME_INDEX to 0.\")\n",
        "        FRAME_INDEX = 0\n",
        "else:\n",
        "    loaded_video = media.read_video(str(resource_path))\n",
        "    original_video_frames = [_ensure_rgb_uint8(f) for f in loaded_video]\n",
        "    source_kind = \"video\"\n",
        "\n",
        "if len(original_video_frames) == 0:\n",
        "    raise ValueError(\"No frames found in resource\")\n",
        "\n",
        "# Preprocess all frames: RGB-only + resize longest side to 1008.\n",
        "video_frames = [_resize_longest_side(f, target_longest=1008) for f in original_video_frames]\n",
        "\n",
        "# Save preprocessed frames into a temporary JPEG folder and run SAM3 on that folder.\n",
        "preprocessed_dir = Path(tempfile.mkdtemp(prefix=\"sam3_preprocessed_frames_\"))\n",
        "for idx, frm in enumerate(video_frames):\n",
        "    Image.fromarray(frm).save(preprocessed_dir / f\"{idx:06d}.jpg\", quality=95)\n",
        "model_resource_path = preprocessed_dir\n",
        "\n",
        "video_frames = np.stack(video_frames, axis=0)\n",
        "original_video_frames = np.stack(original_video_frames, axis=0)\n",
        "\n",
        "num_frames = len(video_frames)\n",
        "if not (0 <= FRAME_INDEX < num_frames):\n",
        "    raise ValueError(f\"FRAME_INDEX must be in [0, {num_frames - 1}], got {FRAME_INDEX}\")\n",
        "\n",
        "frame = np.asarray(video_frames[FRAME_INDEX], dtype=np.uint8)\n",
        "height, width = frame.shape[:2]\n",
        "orig_h, orig_w = original_video_frames[FRAME_INDEX].shape[:2]\n",
        "\n",
        "print(f\"Loaded {source_kind}: {resource_path}\")\n",
        "print(f\"Total frames: {num_frames}\")\n",
        "print(f\"Selected frame: {FRAME_INDEX}\")\n",
        "print(f\"Prompt/model frame size (width x height): {width} x {height}\")\n",
        "print(f\"Original frame size (width x height): {orig_w} x {orig_h}\")\n",
        "print(f\"Model resource path: {model_resource_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08e7118c",
      "metadata": {},
      "outputs": [],
      "source": [
        "annotations_by_obj = {}\n",
        "history = []  # list[tuple[str, int]] where tuple = (kind, obj_id)\n",
        "mode_state = {\"value\": \"box\"}\n",
        "active_obj_state = {\"id\": int(DEFAULT_OBJECT_ID)}\n",
        "rng = np.random.default_rng()\n",
        "\n",
        "\n",
        "def _random_color_rgb():\n",
        "    # Keep colors bright so masks are visible on most videos.\n",
        "    color = rng.integers(40, 256, size=3, dtype=np.int32)\n",
        "    return tuple(int(c) for c in color)\n",
        "\n",
        "\n",
        "def _ensure_object(obj_id):\n",
        "    obj_id = int(obj_id)\n",
        "    if obj_id not in annotations_by_obj:\n",
        "        annotations_by_obj[obj_id] = {\n",
        "            \"boxes_xyxy\": [],\n",
        "            \"box_labels\": [],  # 1 = positive box prompt\n",
        "            \"points_xy\": [],\n",
        "            \"point_labels\": [],  # 1 = positive point, 0 = negative point\n",
        "            \"text_prompt\": \"\",\n",
        "            \"color_rgb\": _random_color_rgb(),\n",
        "        }\n",
        "    return annotations_by_obj[obj_id]\n",
        "\n",
        "\n",
        "def _clip_xy(x, y):\n",
        "    x = int(round(max(0, min(x, width - 1))))\n",
        "    y = int(round(max(0, min(y, height - 1))))\n",
        "    return x, y\n",
        "\n",
        "\n",
        "def _mpl_color(rgb):\n",
        "    return tuple(c / 255.0 for c in rgb)\n",
        "\n",
        "\n",
        "def _refresh_object_output():\n",
        "    with object_output:\n",
        "        clear_output(wait=True)\n",
        "        if not annotations_by_obj:\n",
        "            print(\"No objects yet.\")\n",
        "            return\n",
        "\n",
        "        print(f\"Active object: {active_obj_state['id']}\")\n",
        "        print(\"Objects:\")\n",
        "        for obj_id in sorted(annotations_by_obj.keys()):\n",
        "            obj = annotations_by_obj[obj_id]\n",
        "            color = obj[\"color_rgb\"]\n",
        "            text = obj[\"text_prompt\"] or \"<none>\"\n",
        "            print(\n",
        "                f\"  id={obj_id} color={color} boxes={len(obj['boxes_xyxy'])} \"\n",
        "                f\"points={len(obj['points_xy'])} text={text}\"\n",
        "            )\n",
        "\n",
        "\n",
        "def _render_annotations():\n",
        "    ax.clear()\n",
        "    ax.imshow(frame)\n",
        "\n",
        "    for obj_id in sorted(annotations_by_obj.keys()):\n",
        "        obj = annotations_by_obj[obj_id]\n",
        "        color = _mpl_color(obj[\"color_rgb\"])\n",
        "\n",
        "        for box in obj[\"boxes_xyxy\"]:\n",
        "            x1, y1, x2, y2 = box\n",
        "            ax.add_patch(\n",
        "                Rectangle(\n",
        "                    (x1, y1),\n",
        "                    x2 - x1,\n",
        "                    y2 - y1,\n",
        "                    fill=False,\n",
        "                    edgecolor=color,\n",
        "                    linewidth=2,\n",
        "                )\n",
        "            )\n",
        "            ax.text(\n",
        "                x1,\n",
        "                max(0, y1 - 5),\n",
        "                f\"id={obj_id}\",\n",
        "                color=\"white\",\n",
        "                fontsize=9,\n",
        "                bbox={\"facecolor\": color, \"alpha\": 0.8, \"pad\": 2},\n",
        "            )\n",
        "\n",
        "        if obj[\"points_xy\"]:\n",
        "            points = np.asarray(obj[\"points_xy\"], dtype=np.float32)\n",
        "            labels = np.asarray(obj[\"point_labels\"], dtype=np.int32)\n",
        "\n",
        "            pos_points = points[labels == 1]\n",
        "            neg_points = points[labels == 0]\n",
        "\n",
        "            if len(pos_points) > 0:\n",
        "                ax.scatter(\n",
        "                    pos_points[:, 0],\n",
        "                    pos_points[:, 1],\n",
        "                    c=[color],\n",
        "                    marker=\"o\",\n",
        "                    s=60,\n",
        "                    linewidths=1,\n",
        "                    edgecolors=\"black\",\n",
        "                )\n",
        "            if len(neg_points) > 0:\n",
        "                ax.scatter(\n",
        "                    neg_points[:, 0],\n",
        "                    neg_points[:, 1],\n",
        "                    c=[color],\n",
        "                    marker=\"x\",\n",
        "                    s=70,\n",
        "                    linewidths=2,\n",
        "                )\n",
        "\n",
        "    total_boxes = sum(len(obj[\"boxes_xyxy\"]) for obj in annotations_by_obj.values())\n",
        "    total_points = sum(len(obj[\"points_xy\"]) for obj in annotations_by_obj.values())\n",
        "    total_text = sum(1 for obj in annotations_by_obj.values() if obj[\"text_prompt\"].strip())\n",
        "\n",
        "    ax.set_axis_off()\n",
        "    ax.set_title(\n",
        "        f\"Mode: {mode_state['value']} | Active id: {active_obj_state['id']} | \"\n",
        "        f\"Objects: {len(annotations_by_obj)} | Boxes: {total_boxes} | \"\n",
        "        f\"Points: {total_points} | Text prompts: {total_text}\"\n",
        "    )\n",
        "    fig.canvas.draw_idle()\n",
        "\n",
        "\n",
        "def _set_mode(new_mode):\n",
        "    mode_state[\"value\"] = new_mode\n",
        "    selector.set_active(new_mode == \"box\")\n",
        "    mode_toggle.value = new_mode\n",
        "    _render_annotations()\n",
        "\n",
        "\n",
        "def _select_or_create_object(_=None):\n",
        "    obj_id = int(object_id_input.value)\n",
        "    active_obj_state[\"id\"] = obj_id\n",
        "    obj = _ensure_object(obj_id)\n",
        "    object_text_input.value = obj[\"text_prompt\"]\n",
        "    _refresh_object_output()\n",
        "    _render_annotations()\n",
        "\n",
        "\n",
        "def _set_object_text(_=None):\n",
        "    obj_id = active_obj_state[\"id\"]\n",
        "    obj = _ensure_object(obj_id)\n",
        "    obj[\"text_prompt\"] = object_text_input.value.strip()\n",
        "    history.append((\"text\", obj_id))\n",
        "    _refresh_object_output()\n",
        "    _render_annotations()\n",
        "\n",
        "\n",
        "def _undo_last(_=None):\n",
        "    if not history:\n",
        "        print(\"Nothing to undo.\")\n",
        "        return\n",
        "\n",
        "    kind, obj_id = history.pop()\n",
        "    obj = annotations_by_obj.get(obj_id)\n",
        "    if obj is None:\n",
        "        return\n",
        "\n",
        "    if kind == \"box\" and obj[\"boxes_xyxy\"]:\n",
        "        obj[\"boxes_xyxy\"].pop()\n",
        "        obj[\"box_labels\"].pop()\n",
        "    elif kind == \"point\" and obj[\"points_xy\"]:\n",
        "        obj[\"points_xy\"].pop()\n",
        "        obj[\"point_labels\"].pop()\n",
        "    elif kind == \"text\":\n",
        "        obj[\"text_prompt\"] = \"\"\n",
        "\n",
        "    _refresh_object_output()\n",
        "    _render_annotations()\n",
        "\n",
        "\n",
        "def _clear_active_object(_=None):\n",
        "    obj_id = active_obj_state[\"id\"]\n",
        "    obj = _ensure_object(obj_id)\n",
        "    obj[\"boxes_xyxy\"].clear()\n",
        "    obj[\"box_labels\"].clear()\n",
        "    obj[\"points_xy\"].clear()\n",
        "    obj[\"point_labels\"].clear()\n",
        "    obj[\"text_prompt\"] = \"\"\n",
        "\n",
        "    history[:] = [entry for entry in history if entry[1] != obj_id]\n",
        "    object_text_input.value = \"\"\n",
        "    _refresh_object_output()\n",
        "    _render_annotations()\n",
        "\n",
        "\n",
        "def _clear_all_objects(_=None):\n",
        "    annotations_by_obj.clear()\n",
        "    history.clear()\n",
        "    active_obj_state[\"id\"] = int(DEFAULT_OBJECT_ID)\n",
        "    _ensure_object(active_obj_state[\"id\"])\n",
        "    object_id_input.value = active_obj_state[\"id\"]\n",
        "    object_text_input.value = \"\"\n",
        "    _refresh_object_output()\n",
        "    _render_annotations()\n",
        "\n",
        "\n",
        "def _on_select(eclick, erelease):\n",
        "    if mode_state[\"value\"] != \"box\":\n",
        "        return\n",
        "\n",
        "    if None in (eclick.xdata, eclick.ydata, erelease.xdata, erelease.ydata):\n",
        "        print(\"Selection ignored: drag fully inside the image.\")\n",
        "        return\n",
        "\n",
        "    x1, y1 = _clip_xy(min(eclick.xdata, erelease.xdata), min(eclick.ydata, erelease.ydata))\n",
        "    x2, y2 = _clip_xy(max(eclick.xdata, erelease.xdata), max(eclick.ydata, erelease.ydata))\n",
        "\n",
        "    if x2 <= x1 or y2 <= y1:\n",
        "        print(\"Invalid box (zero/negative area). Try again.\")\n",
        "        return\n",
        "\n",
        "    obj = _ensure_object(active_obj_state[\"id\"])\n",
        "    obj[\"boxes_xyxy\"].append((x1, y1, x2, y2))\n",
        "    obj[\"box_labels\"].append(1)\n",
        "    history.append((\"box\", active_obj_state[\"id\"]))\n",
        "    _refresh_object_output()\n",
        "    _render_annotations()\n",
        "\n",
        "\n",
        "def _on_click(event):\n",
        "    if event.inaxes != ax:\n",
        "        return\n",
        "    if mode_state[\"value\"] not in {\"point+\", \"point-\"}:\n",
        "        return\n",
        "    if event.xdata is None or event.ydata is None:\n",
        "        return\n",
        "\n",
        "    x, y = _clip_xy(event.xdata, event.ydata)\n",
        "    label = 1 if mode_state[\"value\"] == \"point+\" else 0\n",
        "\n",
        "    obj = _ensure_object(active_obj_state[\"id\"])\n",
        "    obj[\"points_xy\"].append((x, y))\n",
        "    obj[\"point_labels\"].append(label)\n",
        "    history.append((\"point\", active_obj_state[\"id\"]))\n",
        "    _refresh_object_output()\n",
        "    _render_annotations()\n",
        "\n",
        "\n",
        "def _on_key(event):\n",
        "    key = (event.key or \"\").lower()\n",
        "    if key == \"b\":\n",
        "        _set_mode(\"box\")\n",
        "    elif key == \"p\":\n",
        "        _set_mode(\"point+\")\n",
        "    elif key == \"n\":\n",
        "        _set_mode(\"point-\")\n",
        "    elif key == \"u\":\n",
        "        _undo_last()\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 7))\n",
        "selector = RectangleSelector(\n",
        "    ax,\n",
        "    _on_select,\n",
        "    useblit=True,\n",
        "    button=[1],\n",
        "    minspanx=5,\n",
        "    minspany=5,\n",
        "    spancoords=\"pixels\",\n",
        "    interactive=True,\n",
        ")\n",
        "\n",
        "fig.canvas.mpl_connect(\"button_press_event\", _on_click)\n",
        "fig.canvas.mpl_connect(\"key_press_event\", _on_key)\n",
        "\n",
        "mode_toggle = widgets.ToggleButtons(\n",
        "    options=[(\"Box\", \"box\"), (\"Positive Point\", \"point+\"), (\"Negative Point\", \"point-\")],\n",
        "    value=\"box\",\n",
        "    description=\"Mode:\",\n",
        ")\n",
        "\n",
        "\n",
        "def _on_mode_toggle(change):\n",
        "    if change[\"name\"] == \"value\" and change[\"new\"] != mode_state[\"value\"]:\n",
        "        _set_mode(change[\"new\"])\n",
        "\n",
        "\n",
        "mode_toggle.observe(_on_mode_toggle)\n",
        "\n",
        "object_id_input = widgets.IntText(value=int(DEFAULT_OBJECT_ID), description=\"Object ID:\")\n",
        "select_object_button = widgets.Button(description=\"Select/Create Object\", button_style=\"info\")\n",
        "select_object_button.on_click(_select_or_create_object)\n",
        "\n",
        "object_text_input = widgets.Text(\n",
        "    description=\"Text:\",\n",
        "    placeholder=\"Object-specific text prompt\",\n",
        "    layout=widgets.Layout(width=\"550px\"),\n",
        ")\n",
        "set_text_button = widgets.Button(description=\"Set Object Text\", button_style=\"success\")\n",
        "set_text_button.on_click(_set_object_text)\n",
        "\n",
        "undo_button = widgets.Button(description=\"Undo\", button_style=\"\")\n",
        "clear_active_button = widgets.Button(description=\"Clear Active Object\", button_style=\"warning\")\n",
        "clear_all_button = widgets.Button(description=\"Clear All Objects\", button_style=\"danger\")\n",
        "undo_button.on_click(_undo_last)\n",
        "clear_active_button.on_click(_clear_active_object)\n",
        "clear_all_button.on_click(_clear_all_objects)\n",
        "\n",
        "object_output = widgets.Output()\n",
        "\n",
        "control_row_1 = widgets.HBox([mode_toggle, undo_button, clear_active_button, clear_all_button])\n",
        "control_row_2 = widgets.HBox([object_id_input, select_object_button])\n",
        "control_row_3 = widgets.HBox([object_text_input, set_text_button])\n",
        "\n",
        "display(control_row_1)\n",
        "display(control_row_2)\n",
        "display(control_row_3)\n",
        "display(object_output)\n",
        "\n",
        "_ensure_object(active_obj_state[\"id\"])\n",
        "_select_or_create_object()\n",
        "_render_annotations()\n",
        "print(\"Interactive multi-object annotator ready.\")\n",
        "print(\"For each object, set text + box first on the seed frame.\")\n",
        "print(\"Then optionally add temporal positive/negative points on any frame via add_temporal_point_prompt(...).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "355321ea",
      "metadata": {},
      "outputs": [],
      "source": [
        "def xyxy_pixels_to_xywh_norm(box_xyxy, image_width, image_height):\n",
        "    x1, y1, x2, y2 = [float(v) for v in box_xyxy]\n",
        "    if x2 <= x1 or y2 <= y1:\n",
        "        raise ValueError(f\"Invalid XYXY box with non-positive size: {box_xyxy}\")\n",
        "    return [\n",
        "        x1 / image_width,\n",
        "        y1 / image_height,\n",
        "        (x2 - x1) / image_width,\n",
        "        (y2 - y1) / image_height,\n",
        "    ]\n",
        "\n",
        "\n",
        "def points_pixels_to_norm(points_xy, image_width, image_height):\n",
        "    if len(points_xy) == 0:\n",
        "        return []\n",
        "    points = np.asarray(points_xy, dtype=np.float32)\n",
        "    points[:, 0] /= image_width\n",
        "    points[:, 1] /= image_height\n",
        "    return points.tolist()\n",
        "\n",
        "\n",
        "# Optional per-frame refinement points, separate from the interactive seed-frame points.\n",
        "# Structure:\n",
        "# { obj_id: [ {\"frame_index\": int, \"xy\": (x, y), \"label\": 0|1, \"source\": str}, ... ] }\n",
        "if \"temporal_point_prompts_by_obj\" not in globals():\n",
        "    temporal_point_prompts_by_obj = {}\n",
        "\n",
        "\n",
        "def add_temporal_point_prompt(obj_id, frame_index, x, y, label=1, source=\"manual\"):\n",
        "    obj_id = int(obj_id)\n",
        "    frame_index = int(frame_index)\n",
        "    if not (0 <= frame_index < num_frames):\n",
        "        raise ValueError(f\"frame_index must be in [0, {num_frames - 1}], got {frame_index}\")\n",
        "\n",
        "    x, y = _clip_xy(x, y)\n",
        "    label = int(label)\n",
        "    if label not in {0, 1}:\n",
        "        raise ValueError(f\"label must be 0 (negative) or 1 (positive), got {label}\")\n",
        "\n",
        "    temporal_point_prompts_by_obj.setdefault(obj_id, []).append(\n",
        "        {\n",
        "            \"frame_index\": frame_index,\n",
        "            \"xy\": (x, y),\n",
        "            \"label\": label,\n",
        "            \"source\": str(source),\n",
        "        }\n",
        "    )\n",
        "\n",
        "\n",
        "def add_temporal_points_from_array(obj_id, frame_indices, points_xy, label=0, source=\"array\"):\n",
        "    if len(frame_indices) != len(points_xy):\n",
        "        raise ValueError(\"frame_indices and points_xy must have equal length\")\n",
        "    for fi, (x, y) in zip(frame_indices, points_xy):\n",
        "        add_temporal_point_prompt(obj_id=obj_id, frame_index=fi, x=x, y=y, label=label, source=source)\n",
        "\n",
        "\n",
        "def clear_temporal_point_prompts(obj_id=None):\n",
        "    if obj_id is None:\n",
        "        temporal_point_prompts_by_obj.clear()\n",
        "        return\n",
        "    temporal_point_prompts_by_obj.pop(int(obj_id), None)\n",
        "\n",
        "\n",
        "def build_object_prompt_plan(strict=True):\n",
        "    object_plans = []\n",
        "    invalid_objects = []\n",
        "\n",
        "    for obj_id in sorted(annotations_by_obj.keys()):\n",
        "        obj = annotations_by_obj[obj_id]\n",
        "        has_boxes = len(obj[\"boxes_xyxy\"]) > 0\n",
        "        has_seed_points = len(obj[\"points_xy\"]) > 0\n",
        "        has_temporal_points = len(temporal_point_prompts_by_obj.get(int(obj_id), [])) > 0\n",
        "        has_points = has_seed_points or has_temporal_points\n",
        "        has_text = bool(obj[\"text_prompt\"].strip())\n",
        "        has_any_prompt = has_boxes or has_points or has_text\n",
        "\n",
        "        if not has_any_prompt:\n",
        "            continue\n",
        "\n",
        "        # Required baseline combination:\n",
        "        # - box + text (stage 1)\n",
        "        # Optional refinement:\n",
        "        # - positive/negative points (stage 2)\n",
        "        is_valid = has_boxes or has_text\n",
        "        if not is_valid:\n",
        "            invalid_objects.append(\n",
        "                f\"obj_id={obj_id} invalid: provide (bounding box + text)\"\n",
        "            )\n",
        "            continue\n",
        "\n",
        "        normal_payload = None\n",
        "        if has_text:\n",
        "            normal_payload = {\n",
        "                \"obj_id\": int(obj_id),\n",
        "                \"frame_index\": FRAME_INDEX,\n",
        "                \"text\": obj[\"text_prompt\"].strip(),\n",
        "            }\n",
        "\n",
        "            if has_boxes:\n",
        "                normal_payload[\"bounding_boxes\"] = [\n",
        "                    xyxy_pixels_to_xywh_norm(box, width, height)\n",
        "                    for box in obj[\"boxes_xyxy\"]\n",
        "                ]\n",
        "                normal_payload[\"bounding_box_labels\"] = obj[\"box_labels\"].copy()\n",
        "\n",
        "        per_frame_points = {}\n",
        "\n",
        "        # Seed-frame points from the interactive annotator.\n",
        "        for (x, y), lbl in zip(obj[\"points_xy\"], obj[\"point_labels\"]):\n",
        "            per_frame_points.setdefault(int(FRAME_INDEX), []).append(((x, y), int(lbl)))\n",
        "\n",
        "        # Additional temporal points (can be on any frame).\n",
        "        for p in temporal_point_prompts_by_obj.get(int(obj_id), []):\n",
        "            fi = int(p[\"frame_index\"])\n",
        "            xy = tuple(p[\"xy\"])\n",
        "            lbl = int(p[\"label\"])\n",
        "            per_frame_points.setdefault(fi, []).append((xy, lbl))\n",
        "\n",
        "        point_payloads = []\n",
        "        for frame_idx in sorted(per_frame_points.keys()):\n",
        "            pts = [xy for (xy, _lbl) in per_frame_points[frame_idx]]\n",
        "            lbls = [int(_lbl) for (_xy, _lbl) in per_frame_points[frame_idx]]\n",
        "            if not pts:\n",
        "                continue\n",
        "            points_norm = points_pixels_to_norm(pts, width, height)\n",
        "            point_payloads.append(\n",
        "                {\n",
        "                    \"obj_id\": int(obj_id),\n",
        "                    \"frame_index\": int(frame_idx),\n",
        "                    \"points\": torch.tensor(points_norm, dtype=torch.float32),\n",
        "                    \"point_labels\": torch.tensor(lbls, dtype=torch.int32),\n",
        "                }\n",
        "            )\n",
        "\n",
        "        object_plans.append(\n",
        "            {\n",
        "                \"obj_id\": int(obj_id),\n",
        "                \"color_rgb\": tuple(obj[\"color_rgb\"]),\n",
        "                \"normal_payload\": normal_payload,\n",
        "                \"point_payloads\": point_payloads,\n",
        "            }\n",
        "        )\n",
        "\n",
        "    if strict and invalid_objects:\n",
        "        raise ValueError(\"\\n\".join(invalid_objects))\n",
        "\n",
        "    return object_plans, invalid_objects\n",
        "\n",
        "\n",
        "object_plans, invalid_objects = build_object_prompt_plan(strict=False)\n",
        "print(f\"Prepared prompt plan for {len(object_plans)} object(s)\")\n",
        "\n",
        "if invalid_objects:\n",
        "    print(\"Invalid objects (not sent to SAM3 unless fixed):\")\n",
        "    for message in invalid_objects:\n",
        "        print(f\"  - {message}\")\n",
        "\n",
        "for plan in object_plans:\n",
        "    obj_id = plan[\"obj_id\"]\n",
        "    print(f\"obj_id={obj_id}\")\n",
        "\n",
        "    if plan[\"normal_payload\"] is not None:\n",
        "        print(f\"    text: {plan['normal_payload']['text']}\")\n",
        "        if plan[\"normal_payload\"].get(\"bounding_boxes\", None) is not None:\n",
        "            first_box = plan[\"normal_payload\"][\"bounding_boxes\"][0]\n",
        "            print(\"  stage 1: normal propagation seed (box + text)\")\n",
        "            print(f\"    first box xywh(norm): {first_box}\")\n",
        "    else:\n",
        "        print(\"  stage 1: no normal prompt for this object\")\n",
        "\n",
        "    if plan[\"point_payloads\"]:\n",
        "        total_points = sum(len(p[\"points\"]) for p in plan[\"point_payloads\"])\n",
        "        frames = [int(p[\"frame_index\"]) for p in plan[\"point_payloads\"]]\n",
        "        first_payload = plan[\"point_payloads\"][0]\n",
        "        first_point = first_payload[\"points\"][0].tolist()\n",
        "        first_label = int(first_payload[\"point_labels\"][0].item())\n",
        "        print(\"  stage 2: rerun with temporal point refinement (positive + negative)\")\n",
        "        print(f\"    refinement frames: {frames}\")\n",
        "        print(f\"    total refinement points: {total_points}\")\n",
        "        print(f\"    first point(norm): {first_point}, label={first_label}\")\n",
        "    else:\n",
        "        print(\"  stage 2: no refinement points (rerun still happens)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e45fef45",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: add temporal refinement points (positive/negative) on arbitrary frames.\n",
        "# You do NOT need to define every prompt at FRAME_INDEX.\n",
        "#\n",
        "# Example manual points:\n",
        "# add_temporal_point_prompt(obj_id=1, frame_index=45, x=420, y=220, label=1, source=\"manual_pos\")\n",
        "# add_temporal_point_prompt(obj_id=1, frame_index=45, x=520, y=250, label=0, source=\"manual_neg\")\n",
        "#\n",
        "# Example from a saved EE pixel array from scripts/extract_orcahand_fingertip_bboxes.py:\n",
        "ee_px = np.load(\"/data/sam3_based_labeling_pipeline/assets/videos/tests/episode_0_oakd_wrist_view_ee_pixels.npy\")  # shape [T,2], -1 for invalid\n",
        "valid_idx = np.where((ee_px[:, 0] >= 0) & (ee_px[:, 1] >= 0))[0]\n",
        "\n",
        "add_temporal_points_from_array(\n",
        "    obj_id=0,\n",
        "    frame_indices=valid_idx.tolist(),\n",
        "    points_xy=ee_px[valid_idx].tolist(),\n",
        "    label=1,\n",
        "    source=\"ee_projection\",\n",
        ")\n",
        "#\n",
        "# Inspect currently queued temporal prompts:\n",
        "print(\n",
        "    {\n",
        "        int(obj_id): len(prompts)\n",
        "        for obj_id, prompts in temporal_point_prompts_by_obj.items()\n",
        "    }\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef70f307",
      "metadata": {},
      "outputs": [],
      "source": [
        "def _xywh_norm_to_xyxy_pixels(boxes_xywh, image_width, image_height):\n",
        "    boxes_xyxy = np.zeros_like(boxes_xywh, dtype=np.float32)\n",
        "    boxes_xyxy[:, 0] = boxes_xywh[:, 0] * image_width\n",
        "    boxes_xyxy[:, 1] = boxes_xywh[:, 1] * image_height\n",
        "    boxes_xyxy[:, 2] = (boxes_xywh[:, 0] + boxes_xywh[:, 2]) * image_width\n",
        "    boxes_xyxy[:, 3] = (boxes_xywh[:, 1] + boxes_xywh[:, 3]) * image_height\n",
        "    return boxes_xyxy\n",
        "\n",
        "\n",
        "def _to_binary_mask(mask):\n",
        "    if isinstance(mask, torch.Tensor):\n",
        "        mask = mask.detach().cpu().numpy()\n",
        "    mask = np.asarray(mask)\n",
        "    while mask.ndim > 2:\n",
        "        mask = mask[0]\n",
        "    return mask > 0.0\n",
        "\n",
        "\n",
        "def _blend_masks_on_frame(raw_frame, frame_outputs, object_colors_rgb, alpha=0.45):\n",
        "    raw_frame = _ensure_rgb_uint8(raw_frame)\n",
        "    overlay = raw_frame.astype(np.float32).copy()\n",
        "    if frame_outputs is None or len(frame_outputs[\"object_ids\"]) == 0:\n",
        "        return raw_frame\n",
        "\n",
        "    out_h, out_w = raw_frame.shape[:2]\n",
        "\n",
        "    for idx, obj_id in enumerate(frame_outputs[\"object_ids\"]):\n",
        "        obj_id = int(obj_id)\n",
        "        color = np.asarray(object_colors_rgb.get(obj_id, (255, 255, 0)), dtype=np.float32)\n",
        "        mask = _to_binary_mask(frame_outputs[\"masks\"][idx])\n",
        "\n",
        "        if mask.shape != (out_h, out_w):\n",
        "            mask_img = Image.fromarray((mask.astype(np.uint8) * 255), mode=\"L\")\n",
        "            mask = np.asarray(\n",
        "                mask_img.resize((out_w, out_h), resample=Image.NEAREST), dtype=np.uint8\n",
        "            ) > 0\n",
        "\n",
        "        if np.any(mask):\n",
        "            overlay[mask] = (1.0 - alpha) * overlay[mask] + alpha * color\n",
        "\n",
        "    return np.clip(overlay, 0, 255).astype(np.uint8)\n",
        "\n",
        "\n",
        "def _run_propagation(session_id, start_frame_index, max_frame_num_to_track, forced_obj_id=None):\n",
        "    # Some predictor backends stream multiple responses per frame (often one object at a time).\n",
        "    # Aggregate by frame+object-id so multi-object results are not overwritten.\n",
        "    # If forced_obj_id is set, remap all streamed masks on a frame to that object id\n",
        "    # (pick the highest-score mask if multiple masks are present).\n",
        "    outputs_by_frame_obj = {}\n",
        "\n",
        "    for stream_response in predictor.handle_stream_request(\n",
        "        request=dict(\n",
        "            type=\"propagate_in_video\",\n",
        "            session_id=session_id,\n",
        "            start_frame_index=start_frame_index,\n",
        "            max_frame_num_to_track=max_frame_num_to_track,\n",
        "        )\n",
        "    ):\n",
        "        out = stream_response[\"outputs\"]\n",
        "        frame_idx = int(stream_response[\"frame_index\"])\n",
        "\n",
        "        obj_ids = np.asarray(out[\"out_obj_ids\"])\n",
        "        masks = np.asarray(out[\"out_binary_masks\"])\n",
        "        scores = np.asarray(out[\"out_probs\"], dtype=np.float32)\n",
        "        boxes_xywh = np.asarray(out[\"out_boxes_xywh\"], dtype=np.float32)\n",
        "        boxes_xyxy = _xywh_norm_to_xyxy_pixels(boxes_xywh, width, height)\n",
        "\n",
        "        frame_store = outputs_by_frame_obj.setdefault(frame_idx, {})\n",
        "\n",
        "        if forced_obj_id is None:\n",
        "            for i, oid in enumerate(obj_ids):\n",
        "                frame_store[int(oid)] = {\n",
        "                    \"mask\": masks[i],\n",
        "                    \"score\": float(scores[i]),\n",
        "                    \"box_xyxy\": boxes_xyxy[i],\n",
        "                }\n",
        "        else:\n",
        "            if len(obj_ids) == 0:\n",
        "                continue\n",
        "            best_idx = int(np.argmax(scores))\n",
        "            frame_store[int(forced_obj_id)] = {\n",
        "                \"mask\": masks[best_idx],\n",
        "                \"score\": float(scores[best_idx]),\n",
        "                \"box_xyxy\": boxes_xyxy[best_idx],\n",
        "            }\n",
        "\n",
        "    outputs = {}\n",
        "    for frame_idx, obj_map in outputs_by_frame_obj.items():\n",
        "        sorted_obj_ids = np.asarray(sorted(obj_map.keys()), dtype=np.int32)\n",
        "        outputs[frame_idx] = {\n",
        "            \"object_ids\": sorted_obj_ids,\n",
        "            \"masks\": np.asarray([obj_map[int(oid)][\"mask\"] for oid in sorted_obj_ids]),\n",
        "            \"scores\": np.asarray([obj_map[int(oid)][\"score\"] for oid in sorted_obj_ids], dtype=np.float32),\n",
        "            \"boxes_xyxy\": np.asarray([obj_map[int(oid)][\"box_xyxy\"] for oid in sorted_obj_ids], dtype=np.float32),\n",
        "        }\n",
        "\n",
        "    return outputs\n",
        "\n",
        "\n",
        "def _merge_outputs_per_frame(dst_outputs, src_outputs):\n",
        "    for frame_idx, src in src_outputs.items():\n",
        "        if frame_idx not in dst_outputs:\n",
        "            dst_outputs[frame_idx] = {\n",
        "                \"object_ids\": np.asarray(src[\"object_ids\"]).copy(),\n",
        "                \"masks\": np.asarray(src[\"masks\"]).copy(),\n",
        "                \"scores\": np.asarray(src[\"scores\"]).copy(),\n",
        "                \"boxes_xyxy\": np.asarray(src[\"boxes_xyxy\"]).copy(),\n",
        "            }\n",
        "            continue\n",
        "\n",
        "        dst = dst_outputs[frame_idx]\n",
        "        merged = {}\n",
        "\n",
        "        for i, oid in enumerate(np.asarray(dst[\"object_ids\"])):\n",
        "            merged[int(oid)] = {\n",
        "                \"mask\": np.asarray(dst[\"masks\"])[i],\n",
        "                \"score\": float(np.asarray(dst[\"scores\"])[i]),\n",
        "                \"box_xyxy\": np.asarray(dst[\"boxes_xyxy\"])[i],\n",
        "            }\n",
        "\n",
        "        for i, oid in enumerate(np.asarray(src[\"object_ids\"])):\n",
        "            merged[int(oid)] = {\n",
        "                \"mask\": np.asarray(src[\"masks\"])[i],\n",
        "                \"score\": float(np.asarray(src[\"scores\"])[i]),\n",
        "                \"box_xyxy\": np.asarray(src[\"boxes_xyxy\"])[i],\n",
        "            }\n",
        "\n",
        "        sorted_obj_ids = np.asarray(sorted(merged.keys()), dtype=np.int32)\n",
        "        dst_outputs[frame_idx] = {\n",
        "            \"object_ids\": sorted_obj_ids,\n",
        "            \"masks\": np.asarray([merged[int(oid)][\"mask\"] for oid in sorted_obj_ids]),\n",
        "            \"scores\": np.asarray([merged[int(oid)][\"score\"] for oid in sorted_obj_ids], dtype=np.float32),\n",
        "            \"boxes_xyxy\": np.asarray([merged[int(oid)][\"box_xyxy\"] for oid in sorted_obj_ids], dtype=np.float32),\n",
        "        }\n",
        "\n",
        "\n",
        "object_plans, invalid_objects = build_object_prompt_plan(strict=True)\n",
        "if not object_plans:\n",
        "    raise ValueError(\"No valid object prompts found. Add prompts and try again.\")\n",
        "\n",
        "normal_payloads = [\n",
        "    plan[\"normal_payload\"] for plan in object_plans if plan[\"normal_payload\"] is not None\n",
        "]\n",
        "point_payloads = []\n",
        "for plan in object_plans:\n",
        "    for payload in plan[\"point_payloads\"]:\n",
        "        point_payloads.append(payload)\n",
        "\n",
        "point_payloads.sort(key=lambda p: (int(p[\"frame_index\"]), int(p[\"obj_id\"])))\n",
        "\n",
        "object_colors = {\n",
        "    int(plan[\"obj_id\"]): tuple(plan[\"color_rgb\"])\n",
        "    for plan in object_plans\n",
        "}\n",
        "\n",
        "predictor = build_sam3_video_predictor()\n",
        "start_response = predictor.handle_request(\n",
        "    request=dict(\n",
        "        type=\"start_session\",\n",
        "        resource_path=str(model_resource_path),\n",
        "    )\n",
        ")\n",
        "session_id = start_response[\"session_id\"]\n",
        "\n",
        "# note: in case you already ran one text prompt and now want to switch to another text prompt\n",
        "# it's required to reset the session first (otherwise the results would be wrong)\n",
        "_ = predictor.handle_request(\n",
        "    request=dict(\n",
        "        type=\"reset_session\",\n",
        "        session_id=session_id,\n",
        "    )\n",
        ")\n",
        "\n",
        "if DISABLE_HOLE_FILLING:\n",
        "    predictor.model.fill_hole_area = 0\n",
        "    print(f\"Disabled hole-filling postprocessing ({source_kind} mode).\")\n",
        "\n",
        "# Stage 1: run each semantic prompt separately (text+box resets semantic state in SAM3),\n",
        "# then merge per-object outputs into one multi-object result.\n",
        "first_pass_outputs_per_frame = {}\n",
        "for payload in normal_payloads:\n",
        "    _ = predictor.handle_request(\n",
        "        request=dict(\n",
        "            type=\"reset_session\",\n",
        "            session_id=session_id,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    request = dict(type=\"add_prompt\", session_id=session_id)\n",
        "    request.update(payload)\n",
        "    _ = predictor.handle_request(request=request)\n",
        "\n",
        "    obj_outputs = _run_propagation(\n",
        "        session_id=session_id,\n",
        "        start_frame_index=FRAME_INDEX,\n",
        "        max_frame_num_to_track=MAX_FRAME_NUM_TO_TRACK,\n",
        "        forced_obj_id=int(payload[\"obj_id\"]),\n",
        "    )\n",
        "    _merge_outputs_per_frame(first_pass_outputs_per_frame, obj_outputs)\n",
        "\n",
        "print(\n",
        "    f\"Stage 1 processed {len(first_pass_outputs_per_frame)} frames \"\n",
        "    f\"(normal prompts: {len(normal_payloads)}).\"\n",
        ")\n",
        "\n",
        "# Stage 2: apply per-object refinements by re-running that object and replacing only its track.\n",
        "outputs_per_frame = {\n",
        "    frame_idx: {\n",
        "        \"object_ids\": np.asarray(frame_out[\"object_ids\"]).copy(),\n",
        "        \"masks\": np.asarray(frame_out[\"masks\"]).copy(),\n",
        "        \"scores\": np.asarray(frame_out[\"scores\"]).copy(),\n",
        "        \"boxes_xyxy\": np.asarray(frame_out[\"boxes_xyxy\"]).copy(),\n",
        "    }\n",
        "    for frame_idx, frame_out in first_pass_outputs_per_frame.items()\n",
        "}\n",
        "\n",
        "point_payloads_by_obj = {}\n",
        "for p in point_payloads:\n",
        "    point_payloads_by_obj.setdefault(int(p[\"obj_id\"]), []).append(p)\n",
        "\n",
        "for payload in normal_payloads:\n",
        "    obj_id = int(payload[\"obj_id\"])\n",
        "    obj_point_payloads = point_payloads_by_obj.get(obj_id, [])\n",
        "\n",
        "    _ = predictor.handle_request(\n",
        "        request=dict(\n",
        "            type=\"reset_session\",\n",
        "            session_id=session_id,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    base_request = dict(type=\"add_prompt\", session_id=session_id)\n",
        "    base_request.update(payload)\n",
        "    _ = predictor.handle_request(request=base_request)\n",
        "\n",
        "    for p in obj_point_payloads:\n",
        "        request = dict(type=\"add_prompt\", session_id=session_id)\n",
        "        print(\n",
        "            \"Adding refinement prompt \"\n",
        "            f\"(obj={p['obj_id']} frame={p['frame_index']} n_points={len(p['points'])})\"\n",
        "        )\n",
        "        request.update(p)\n",
        "        _ = predictor.handle_request(request=request)\n",
        "\n",
        "    refined_obj_outputs = _run_propagation(\n",
        "        session_id=session_id,\n",
        "        start_frame_index=FRAME_INDEX,\n",
        "        max_frame_num_to_track=MAX_FRAME_NUM_TO_TRACK,\n",
        "        forced_obj_id=obj_id,\n",
        "    )\n",
        "    _merge_outputs_per_frame(outputs_per_frame, refined_obj_outputs)\n",
        "\n",
        "second_pass_ran = True\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12ed68c1",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\n",
        "    f\"Final propagation processed {len(outputs_per_frame)} frames \"\n",
        "    f\"(second pass ran: {second_pass_ran}, point refinements: {len(point_payloads)}).\"\n",
        ")\n",
        "\n",
        "sorted_indices = sorted(outputs_per_frame.keys())\n",
        "if sorted_indices:\n",
        "    first_ids = [int(x) for x in outputs_per_frame[sorted_indices[0]][\"object_ids\"]]\n",
        "    print(f\"Objects present on first propagated frame: {first_ids}\")\n",
        "overlay_video_frames = []\n",
        "for frame_idx in sorted_indices:\n",
        "    raw_frame = np.asarray(original_video_frames[frame_idx], dtype=np.uint8)\n",
        "    frame_outputs = outputs_per_frame[frame_idx]\n",
        "    overlay_frame = _blend_masks_on_frame(\n",
        "        raw_frame,\n",
        "        frame_outputs,\n",
        "        object_colors_rgb=object_colors,\n",
        "        alpha=MASK_ALPHA,\n",
        "    )\n",
        "    overlay_video_frames.append(overlay_frame)\n",
        "\n",
        "if overlay_video_frames:\n",
        "    first_idx = sorted_indices[0]\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    axes[0].imshow(np.asarray(original_video_frames[first_idx], dtype=np.uint8))\n",
        "    axes[0].set_title(f\"Original frame {first_idx}\")\n",
        "    axes[0].set_axis_off()\n",
        "\n",
        "    axes[1].imshow(overlay_video_frames[0])\n",
        "    axes[1].set_title(f\"Multi-object overlay frame {first_idx}\")\n",
        "    axes[1].set_axis_off()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    media.show_video(overlay_video_frames, fps=OUTPUT_FPS)\n",
        "\n",
        "if OUTPUT_OVERLAY_VIDEO_PATH is None:\n",
        "    default_name = (\n",
        "        resource_path.stem if resource_path.is_file() else resource_path.name\n",
        "    )\n",
        "    OUTPUT_OVERLAY_VIDEO_PATH = str(\n",
        "        resource_path.parent / f\"{default_name}_sam3_multi_object_overlay.mp4\"\n",
        "    )\n",
        "\n",
        "media.write_video(OUTPUT_OVERLAY_VIDEO_PATH, overlay_video_frames, fps=OUTPUT_FPS)\n",
        "print(f\"Saved overlay video to: {OUTPUT_OVERLAY_VIDEO_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5595b8d0",
      "metadata": {},
      "outputs": [],
      "source": [
        "_ = predictor.handle_request(\n",
        "    request=dict(\n",
        "        type=\"close_session\",\n",
        "        session_id=session_id,\n",
        "    )\n",
        ")\n",
        "\n",
        "predictor.shutdown()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b4e1805",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(outputs_per_frame.keys())\n",
        "print(outputs_per_frame[0].keys())\n",
        "print(outputs_per_frame[0][\"object_ids\"])\n",
        "print(outputs_per_frame[0][\"masks\"])\n",
        "print(outputs_per_frame[0][\"scores\"])\n",
        "print(outputs_per_frame[0][\"boxes_xyxy\"])\n",
        "print(outputs_per_frame[0][\"object_ids\"].shape)\n",
        "print(outputs_per_frame[0][\"masks\"].shape)\n",
        "print(outputs_per_frame[0][\"scores\"].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6569133",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "sam3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
